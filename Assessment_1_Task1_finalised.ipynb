{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University of Aberdeen\n",
    "\n",
    "## Applied AI (CS5079)\n",
    "\n",
    "### Assessment 1 Task 1 - Reinforcement Learning from the Screen Frames\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai has preprocessing modules in their baselines repository such as FrameStack, NoopResetEnv, episode_life, etc.\n",
    "!pip install git+https://github.com/openai/baselines.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value used for achieving reproducibility\n",
    "SEED_VALUE = 1337\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "# For handling files\n",
    "import os\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISM'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(SEED_VALUE)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(SEED_VALUE)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED_VALUE)\n",
    "\n",
    "# Import the Keras backend used for freeing the global state\n",
    "# to avoid clutter\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# https://github.com/openai/baselines\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind, NoopResetEnv, FrameStack\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# For plotting graphs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# OpenAI Gym\n",
    "import gym\n",
    "FRAME_STACK_SIZE = 5\n",
    "# NOOP_FRAMES = 10\n",
    "env = gym.make(\"Asterix-v0\")\n",
    "env = wrap_deepmind(env, frame_stack=False, scale=True, clip_rewards=False, episode_life=False)\n",
    "# env = NoopResetEnv(env, noop_max=NOOP_FRAMES)\n",
    "env = FrameStack(env, FRAME_STACK_SIZE)\n",
    "\n",
    "env.seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        action_size of the agent is taken from the environment's action space\n",
    "        \"\"\"\n",
    "        self.action_size = env.action_space.n\n",
    "        \n",
    "    def get_action(self, observation):\n",
    "        return random.choice(range(self.action_size))\n",
    "    \n",
    "total_reward=0\n",
    "agent = RandomAgent(env)\n",
    "numberOfEpisodes = 1\n",
    " \n",
    "for steps in range(numberOfEpisodes):\n",
    "    current_obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(current_obs)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "print(\"Average reward: {}\".format(total_reward/numberOfEpisodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Possible actions:\", env.unwrapped.get_action_meanings())\n",
    "print('Info dictionary:', env.step(action)[3])\n",
    "print('Reward:', env.step(action)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display this, a simple agent that performs random actions for one episode is implemented.\n",
    "\n",
    "Observations: `env.observation_space` shows that the obseravtion space is an **RGB** image of which is an array of shape **(210, 160, 3)**, indicating a height of 210 pixels, a width of 160 pixels and 3 channels.  <br />  <br /> \n",
    "Action Space: `env.action_space` shows us that the action space is **discrete** with **9 possible actions** that are printed out for better understanding using `env.unwrapped.get_action_meanings()`. Each action is performed repeatedly for a duration of ***k*** frames, where ***k*** is sampled from the set {2,3,4} uniformly. The discrete space allows for a fixed range of non-negative numbers, where low (`env.observation_space.low`) will be a grid filled with zeros (**0x0x0**) and high (`env.observation_space.high`) will be a grid filled with **250x250x250**. <br /> <br /> \n",
    "Reward: **0** but is accumulated as an episode is played and the player collects cauldrons and items giving him points. <br /> <br /> \n",
    "Environment's info dictionary: The info dictionary contains `ale.lives()` which refers to the number of lives left. In our case, the player has **only 3 lives** that decrease as he collides with the lyres <br /> <br /> \n",
    "Episode: An episode is concluded when the player loses a life. When a game is played and **done** is equal to **True**, this indicates that the game is finished and the player has lost all lives. <br /> <br /> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the report for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a special wrapper for the purpose of environment preprocessing. \n",
    "\n",
    "This reshapes the image to .... and the agent will receive a grayscale observation as we have converted the observarions to grayscale. We also scale all observations to [0,1].\n",
    "\n",
    "Another common preprocessing step is the introductioon of frame skipping (Naddaf, 2010 at https://era.library.ualberta.ca/items/a661eb66-f2e0-4ed3-b501-b6cbcd1fdd9d), which is what restricts the agent's decision points by repeating some selected action for ***k*** consequitve frames, making the RL problem simpler and speeding up execution. \n",
    "\n",
    "This notebook follows an approach where the agent is designed with a richer observation space by combining past frames with most recent ones as known as frame stacking (Mnih et al., 2015 https://www.nature.com/articles/nature14236). We use ***5*** frames, but the algorithm might also be robust with different values such as 3 or 4. This research experimented with all 3 options and concluded that the use of 5 images in a stack is the most beneficial approach with regards to total reward. Due to this reduction in partial observability, the agent can detect the direction of motion of in-game objects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "from skimage.color import rgb2gray\n",
    "print(env.observation_space.shape)\n",
    "plt.imshow(obs[:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the report for a more indepth explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read this to check architecture and fig\n",
    "\n",
    "https://www.nature.com/articles/nature14236\n",
    "\n",
    "ARCHITECTURE\n",
    "TODO:\n",
    "\n",
    "The first hidden layer convolves 32 filters of 8 × 8 with stride 4 with the input image and applies a rectifier nonlinearity 31,32. The second hidden layer convolves 64 filters of 4 × 4 with stride 2, again followed by a rectifier nonlinearity. This is followed by a third convolutional layer that convolves 64 filters of 3 × 3 with stride 1 followed by a rectifier. The final two hidden layers are fully-connected and consist of 512 and 256 rectifier units respectively. The output layer is a fully-connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the garbage collector package\n",
    "import gc\n",
    "\n",
    "def reset_random_seeds():\n",
    "    \"\"\"Reset the random number generator seed to achieve full reproducibility \n",
    "    even when running the script on GPU\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the environment determinism to guarantee\n",
    "    # reproducibility of the results\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISM'] = '1'\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED_VALUE)\n",
    "    tf.random.set_seed(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    random.seed(SEED_VALUE)\n",
    "    \n",
    "    # Perform garbage collection\n",
    "    gc.collect()\n",
    "    print(\"Random number generator seed reset!\")  # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining global variable and importing packages for training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Keras layer used for building our models\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D,\\\n",
    "MaxPool2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "\n",
    "INPUT_SHAPE = env.observation_space.shape\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 25000\n",
    "EXPLORATION_STEPS = 20000\n",
    "LEARNING_RATE = 0.00025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    \"\"\"Code is adapted from an open-source MIT-licensed GitHub repository.\n",
    "    It implements an optimisation experience replay memory technique that can\n",
    "    be used for all Atari games DQNs. \n",
    "    \n",
    "    Source: https://github.com/rlcode/per\n",
    "    Args:\n",
    "        capacity (int): size of the sum tree\n",
    "    \"\"\"\n",
    "    # The data pointer class variable\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Initialize the tree with all nodes and data with values set to 0.\n",
    "        \"\"\"\n",
    "        # Number of leaf nodes (final nodes) that contains experiences\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        # Store the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        \"\"\"Recursively propagate a given child node change through the sum tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the parent node index\n",
    "        parent = (idx - 1) // 2\n",
    "        \n",
    "        # Apply the child node change to the parent node\n",
    "        self.tree[parent] += change\n",
    "        \n",
    "        # Do this until the root node is reached\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        \"\"\"Return a leaf node (priority) index for the current experience (observation)\n",
    "        \"\"\"\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        \"\"\"Return the root node (total sum of the experience priority values).\n",
    "        \"\"\"\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        \"\"\"Add the experience and its priority score (as a leaf) to the sum tree.\n",
    "        \"\"\"\n",
    "        # Get the tree index for the experience (observation)\n",
    "        idx = self.write + self.capacity - 1\n",
    "        \n",
    "        # Update the sum tree data\n",
    "        self.data[self.write] = data\n",
    "        \n",
    "        # Update the sum tree probability values\n",
    "        self.update(idx, p)\n",
    "        \n",
    "        # Increment the data pointer\n",
    "        self.write += 1\n",
    "        \n",
    "        # If over the capacity, go back to first index (we overwrite)\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        \"\"\"Update the leaf priority score and propagate the change through tree.\n",
    "        \"\"\"\n",
    "        # Change = new priority score - former priority score\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        \n",
    "        # Propagate the change through tree\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        \"\"\"Return the leaf index, priority value and its associated experience (observation).\n",
    "        \"\"\"\n",
    "        # Get the leaf index\n",
    "        idx = self._retrieve(0, s)\n",
    "        \n",
    "        # Get the experience (observation) index\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "    \n",
    "\n",
    "#-------------------- MEMORY --------------------------\n",
    "class Memory:\n",
    "    \"\"\"Code is adapted from an open-source MIT-licensed GitHub repository.\n",
    "    It implements an optimisation experience replay memory technique that can\n",
    "    be used for all Atari games DQNs. \n",
    "    \n",
    "    Source: https://github.com/rlcode/per\n",
    "    \n",
    "    Args:\n",
    "        capacity (int): size of the sum tree\n",
    "    \"\"\"\n",
    "    # Hyperparameter to avoid assigning 0 probability to experiences\n",
    "    e = 0.01\n",
    "    \n",
    "    # Hyperparameter to make a trade-off between random sampling and taking a\n",
    "    # a high priority experience (observation)\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Initialise the sum tree with the given capacity.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        \"\"\"Store a new experience in the tree along with its corresponding priority value.\n",
    "        \"\"\"\n",
    "        # Get the priority from the experience (observation) error\n",
    "        p = self._getPriority(error)\n",
    "        \n",
    "        # Store the priority and the experience (observation)\n",
    "        self.tree.add(p, sample) \n",
    "\n",
    "    def sample(self, n):\n",
    "        \"\"\"Sample a n-sized batch of priority index and observation pair.\n",
    "        \"\"\"\n",
    "        # Create a list to hold the batch pairs\n",
    "        batch = []\n",
    "        \n",
    "        # Calculate and store the priority segment\n",
    "        segment = self.tree.total() / n\n",
    "        \n",
    "        # Populate the batch list\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append( (idx, data) )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        \"\"\"Update the sum tree leaves (priorities).\n",
    "        \"\"\"\n",
    "        # Get the priority from the experience (observation) error\n",
    "        p = self._getPriority(error)\n",
    "        \n",
    "        # Update the sum tree probability values\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import math\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "LAMBDA = - math.log(0.01) / EXPLORATION_STEPS  # speed of decay\n",
    "\n",
    "class DDQNAgent():\n",
    "    \"\"\"Code is adapted from an open-source MIT-licensed GitHub repository.\n",
    "    It implements an DDQ Network and Agent experience replay memory technique that can\n",
    "    be used for all Atari games DQNs. \n",
    "    \n",
    "    Source: https://github.com/jaromiru/AI-blog\n",
    "    \n",
    "    Args:\n",
    "        env (AtariEnv): the environment that RLNetwork and Agent are optimized on\n",
    "        lr (float): learning rate of the network\n",
    "    \"\"\"\n",
    "    steps = 0\n",
    "    \n",
    "    def __init__(self, env, lr=LEARNING_RATE):\n",
    "        \"\"\"Initialise the agent and network for the environment.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.memory = Memory(MEMORY_SIZE)\n",
    "        \n",
    "        self.loss_val = np.inf\n",
    "        self.action_size = NUM_ACTIONS\n",
    "        \n",
    "        # Disable eager execution which boosts runtime\n",
    "        # Eager execution is generally used for debugging purposes\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        self.discount_rate = 0.99\n",
    "\n",
    "        # Create the two networks for predicting the actions\n",
    "        \n",
    "        # The first model makes the predictions for Q-values \n",
    "        # which are used to make a action.\n",
    "        self.online = self.q_network()\n",
    "        \n",
    "        # Build a target model for the prediction of future\n",
    "        # rewards. The weights of a target model get updated \n",
    "        # every 10,000 steps thus when the loss between the \n",
    "        # Q-values is calculated the target Q-value is stable.\n",
    "        self.target = self.q_network()\n",
    "\n",
    "        #The \"target\" DNN will take the values of the \"online\" DNN\n",
    "        self.update_target()\n",
    "\n",
    "\n",
    "    def q_network(self, filters_1=32, filters_2=64, filters_3=64):\n",
    "        \"\"\"Define and return the CNN model architecture.\n",
    "        \"\"\"\n",
    "        # Ensure reproducibility of the results\n",
    "        # by resetting the random seeds\n",
    "        reset_random_seeds()\n",
    "\n",
    "        # Build the model\n",
    "        kernel_init = HeUniform()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters_1, kernel_size=8, padding=\"same\", strides=4, activation='relu', input_shape=INPUT_SHAPE, kernel_initializer=kernel_init))\n",
    "        model.add(Conv2D(filters_2, kernel_size=4, padding=\"same\", strides=2, activation='relu', kernel_initializer=kernel_init))\n",
    "        model.add(Conv2D(filters_3, kernel_size=3, padding=\"same\", strides=1, activation='relu', kernel_initializer=kernel_init))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer=kernel_init))\n",
    "        model.add(Dense(256, activation='relu', kernel_initializer=kernel_init))\n",
    "        model.add(Dense(NUM_ACTIONS, activation=\"linear\", kernel_initializer=kernel_init))\n",
    "        \n",
    "        # In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "        # improves training time\n",
    "        model.compile(loss=\"huber_loss\", optimizer=Adam(learning_rate=self.lr))\n",
    "        return model\n",
    "            \n",
    "    def update_target(self):\n",
    "        \"\"\"Update the target network with the online network weights.\n",
    "        \"\"\"\n",
    "        # Get the online DQN weights\n",
    "        online_weights = self.online.get_weights()\n",
    "        \n",
    "        # Update the target DQN weights\n",
    "        self.target.set_weights(online_weights)\n",
    "        \n",
    "    #---- CHOSSING ACTION ----\n",
    "    def get_action(self, state, step):\n",
    "        \"\"\"Based on the Epsilon agent chooses wheter to explore or exploatate\n",
    "        \"\"\"\n",
    "        if step <= EXPLORATION_STEPS:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:     \n",
    "            self.steps += 1\n",
    "            epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "            if np.random.rand() < epsilon:\n",
    "                return np.random.randint(self.action_size)# random action\n",
    "            else:\n",
    "                q_values = self.online.predict(state.reshape(-1, *INPUT_SHAPE))\n",
    "                return np.argmax(q_values) # optimal action\n",
    "    \n",
    "    def get_train_data(self, batch):\n",
    "        \"\"\"Decide on the input, desired output and error of the current state.\n",
    "        \"\"\"\n",
    "        no_state = np.zeros(INPUT_SHAPE)\n",
    "        \n",
    "        prev_states = np.array([obs[1][0] for obs in batch])\n",
    "        \n",
    "        # No state if done = True\n",
    "        next_states = np.array([(no_state if obs[1][4] is True else obs[1][3]) for obs in batch])\n",
    "    \n",
    "        prev_q_vals = self.online.predict(prev_states)\n",
    "        \n",
    "        next_q_vals_double = self.online.predict(next_states)\n",
    "        next_q_vals = self.target.predict(next_states)\n",
    "        \n",
    "        X = np.zeros((len(batch), *INPUT_SHAPE))\n",
    "        Y = np.zeros((len(batch), NUM_ACTIONS))\n",
    "        errors = np.zeros(len(batch))\n",
    "        \n",
    "        for idx in range(len(batch)):\n",
    "            \n",
    "            # Unpack the current batch sample\n",
    "            curr_state, action, reward, next_state, done = batch[idx][1]\n",
    "            \n",
    "            q_val = prev_q_vals[idx]\n",
    "            prev_q_val = q_val[action]\n",
    "            \n",
    "            # Future q value\n",
    "            future_q_val = q_val\n",
    "            \n",
    "            if done:\n",
    "                future_q_val[action] = reward\n",
    "            else:\n",
    "                future_q_val[action] = reward + next_q_vals[idx][np.argmax(next_q_vals_double[idx])] * self.discount_rate\n",
    "            \n",
    "            X[idx] = curr_state\n",
    "            Y[idx] = future_q_val\n",
    "            errors[idx] = abs(prev_q_val - future_q_val[action])\n",
    "        \n",
    "        return (X, Y, errors)\n",
    "    \n",
    "    def save_to_memory(self, curr_state, action, reward, next_state, done, step):\n",
    "        \"\"\"Update the sum tree priorities and observations (samples).\n",
    "        \"\"\"\n",
    "        sample = (curr_state, action, reward, next_state, done)\n",
    "        if step <= EXPLORATION_STEPS:\n",
    "            error = abs(sample[2])  # Reward\n",
    "            self.memory.add(error, sample)\n",
    "        else:\n",
    "            X, Y, errors = self.get_train_data([(0, sample)])\n",
    "            self.memory.add(errors[0], sample)\n",
    "        \n",
    "    def train(self, step):\n",
    "        \"\"\"Train the online model and update the loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        X, Y, errors = self.get_train_data(batch)\n",
    "        \n",
    "        # Update errors\n",
    "        for i in range(len(batch)):\n",
    "            idx = batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "        \n",
    "        hist = self.online.fit(X, Y, batch_size=BATCH_SIZE, epochs=1, verbose=0, shuffle=True)\n",
    "        self.loss_val = hist.history['loss'][0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the report for further information regarding the deployment and parameter adjustments of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "agent = DDQNAgent(env)  \n",
    "ep_rewards = []\n",
    "ep_steps = []\n",
    "total_reward = 0\n",
    "N_STEPS = 1000000  # total number of training steps\n",
    "save_steps = 50\n",
    "\n",
    "\n",
    "# Train the model after 4 actions\n",
    "TRAIN_ONLINE_STEPS = 4\n",
    "\n",
    "# Update teh target every 10,000 steps (1 epoch)\n",
    "# Considered as a hyperparameter\n",
    "UPDATE_TARGET_STEPS = 10000\n",
    "\n",
    "done=True\n",
    "for step in range(N_STEPS):\n",
    "    total_perc = step * 100 / N_STEPS\n",
    "    print(f\"\\r\\tAction step: {step}/{N_STEPS} ({total_perc:.2f}%)\\tLoss: {agent.loss_val:5f}\", end=\"\")\n",
    "    if done: # game over, start again\n",
    "        avg_reward = int(total_reward/3)\n",
    "        if step:\n",
    "            ep_rewards.append(total_reward)\n",
    "            ep_steps.append(step)\n",
    "        print(f\"\\tAVG reward: {avg_reward}\\tTotal mean: {np.mean(ep_rewards)}\")\n",
    "\n",
    "        obs = env.reset()\n",
    "        state = np.array(obs)\n",
    "        total_reward = 0\n",
    "\n",
    "\n",
    "    # Get a exploration/exploitation action\n",
    "    action = agent.get_action(state, step)\n",
    "\n",
    "    # Take a step in the game environment\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Convert to NumPy array\n",
    "    next_state = np.array(next_state)\n",
    "    \n",
    "    # Update the sum tree priorities and observations (samples)\n",
    "    agent.save_to_memory(state, action, reward, next_state, done, step)\n",
    "    \n",
    "    # Skip training the agent if still exploring\n",
    "    if step > EXPLORATION_STEPS:\n",
    "        \n",
    "        # Train the online DDQN every 4th frame\n",
    "        if step % TRAIN_ONLINE_STEPS == 0:\n",
    "            agent.train(step)\n",
    "\n",
    "        # Regularly copy the online DDQN to the target DDQN\n",
    "        if step % UPDATE_TARGET_STEPS == 0:\n",
    "            agent.update_target()\n",
    "    \n",
    "    env.render()\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# train_df = pd.read_csv('sumtree_ddqn_data.csv')\n",
    "# train_df.iloc[:, 1:]\n",
    "# Create data frame for the obtained training data\n",
    "train_df = pd.DataFrame(data={'Step': ep_steps, 'Reward': ep_rewards})\n",
    "\n",
    "# Calculate cumulative mean of the rewards\n",
    "train_df['Total Mean'] = train_df['Reward'].expanding().mean()\n",
    "\n",
    "# Calculate cumulative sum of the rewards\n",
    "train_df['Total Reward'] = train_df['Reward'].cumsum()\n",
    "train_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting the DDQN training data\n",
    "def plot_df(df, cols, x_label, y_label, title, title_fontsize=20, label_fontsize=16):\n",
    "    plt.rcParams['figure.figsize']= (20, 6)\n",
    "    for col in cols:\n",
    "        df[col].plot(fontsize=12)\n",
    "    plt.legend(loc=2, prop={'size': 14})\n",
    "    plt.xlabel(x_label, fontsize=label_fontsize)\n",
    "    plt.ylabel(y_label, fontsize=label_fontsize)\n",
    "    plt.title(title, fontsize=title_fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(df, cols, models, x_label, y_label, title, title_fontsize=20, label_fontsize=16):\n",
    "    plt.rcParams['figure.figsize']= (20, 6)\n",
    "    plt.xlabel(x_label, fontsize=label_fontsize)\n",
    "    plt.ylabel(y_label, fontsize=label_fontsize)\n",
    "    plt.title(title, fontsize=title_fontsize)\n",
    "    plt.boxplot(df[cols])\n",
    "    \n",
    "    initial_labels = []\n",
    "    final_labels = []\n",
    "    for idx, model in enumerate(models):\n",
    "        initial_labels.append(idx+1)\n",
    "        final_labels.append(model)\n",
    "    plt.xticks(initial_labels, final_labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training reward cumulative sum throughout the episodes (1 episode = 3 player lives)\n",
    "plot_df(train_df, cols=['Total Reward'], x_label='Episode', y_label='Reward', title='DDQN Agent Training Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training reward values throughout the episodes (1 episode = 3 player lives)\n",
    "plot_df(train_df, cols=['Reward', 'Total Mean'], x_label='Episode', y_label='Reward', title='DDQN Agent Training Rewards and Trendline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(train_df, ['Reward'], ['DDQN-Image'], x_label='Current Models', y_label='Reward', title='DDQN Agent Training Rewards BoxPlot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the obtaine DDQN training data to a CSV file\n",
    "train_df.to_csv('sumtree_ddqn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HDF5 file with the trained online model\n",
    "# with all the details necessary to reconstitute it. \n",
    "online_model = agent.online\n",
    "online_model.save('trained_ddqn_online_model.h5') \n",
    "\n",
    "# Create a HDF5 file with the trained target model\n",
    "# with all the details necessary to reconstitute it. \n",
    "target_model = agent.target\n",
    "target_model.save('trained_ddqn_target_model.h5')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
