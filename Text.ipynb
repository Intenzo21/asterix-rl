{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X786bJdxRNm"
      },
      "source": [
        "# University of Aberdeen\n",
        "\n",
        "## Applied AI (CS5079)\n",
        "\n",
        "### Assessment 1 - Asterix\n",
        "\n",
        "#### Part 1\n",
        "---\n",
        "\n",
        "Reinforcement Learning from the Screen Frames\n",
        "\n",
        "###1.1) Imports and Environment set up\n",
        "---\n",
        "This step takes care of all the necessary imports for the flawless execution of the first task. \n",
        "We will be using the standard lbiraries used for machine learning, and statistical/mathematical analysis and plotting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLImLvZ1uYju",
        "outputId": "2e421db7-9ffd-473a-be5b-f88b765562ff"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from math import sqrt\n",
        "from statistics import mean\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1337, 17452119]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn6pKPSTxRYk"
      },
      "source": [
        "\n",
        "The next step is game initialization. As we are using OpenAI to implement our model, we will import the OpenAI Gym module and load the Asterix game. As this is a Reinforcement Learning experiment, the team values empiricism and reproducibility, so we make use of randomness by calling <b>env.seed()</b>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJab_G_s1aEH"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"Asterix-v0\")\n",
        "env.seed(1337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEjb4rVT3HkC"
      },
      "source": [
        "###*Observing the environment*\n",
        "---\n",
        "Our next task is the description of the environment. This includes observation space, which outputs the enviornment type and dimensions.\n",
        "\n",
        "We need action space as this is used to define the characterstics of the environment action space.\n",
        "\n",
        "We then take a quick look at rewards, the environment's info dictionary and episode.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeFUbrgpuYjw",
        "outputId": "21e35667-e904-4dd9-be5e-0186587f623b"
      },
      "source": [
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n",
        "print(\"Possible actions:\", env.unwrapped.get_action_meanings())\n",
        "print('Info dictionary:', env.step(action)[3])\n",
        "print('Reward:', env.step(action)[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box(210, 160, 3)\n",
            "Action space: Discrete(9)\n",
            "Possible actions: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_HfGcym3eCn"
      },
      "source": [
        "\n",
        "Observations: `env.observation_space` shows that the obseravtion space is an **RGB** image of which is an array of shape **(210, 160, 3)**, indicating a height of 210 pixels, a width of 160 pixels and 3 channels.  <br />  <br /> \n",
        "Action Space: `env.action_space` shows us that the action space is **discrete** with **9 possible actions** that are printed out for better understanding using `env.unwrapped.get_action_meanings()`. Each action is performed repeatedly for a duration of ***k*** frames, where ***k*** is sampled from the set {2,3,4} uniformly. The discrete space allows for a fixed range of non-negative numbers, where low (`env.observation_space.low`) will be a grid filled with zeros (**0x0x0**) and high (`env.observation_space.high`) will be a grid filled with **250x250x250**. <br /> <br /> \n",
        "Reward: **0** but is accumulated as an episode is played and the player collects cauldrons and items giving him points. <br /> <br /> \n",
        "Environment's info dictionary: The info dictionary contains `ale.lives()` which refers to the number of lives left. In our case, the player has **only 3 lives** that decrease as he collides with the lyres <br /> <br /> \n",
        "Episode: An episode is concluded when the player loses a life. When a game is played and **done** is equal to **True**, this indicates that the game is finished and the player has lost all lives. <br /> <br /> \n",
        "\n",
        "To display this, a simple agent that performs random actions for one episode is implemented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZb5Pe77uYjx"
      },
      "source": [
        "class RandomAgent():\n",
        "    def __init__(self, env):\n",
        "        self.action_size = env.action_space.n\n",
        "        \n",
        "    def get_action(self, observation):\n",
        "        return random.choice(range(self.action_size))\n",
        "    \n",
        "total_reward=0\n",
        "agent = RandomAgent(env)\n",
        "numberOfEpisodes = 1\n",
        " \n",
        "for steps in range(numberOfEpisodes):\n",
        "     current_obs = env.reset()\n",
        "     done = False\n",
        "     while not done:\n",
        "         action = agent.get_action(current_obs)\n",
        "         next_obs, reward, done, info = env.step(action)\n",
        "         total_reward += reward\n",
        "         env.render()\n",
        "print(\"Average reward: {}\".format(total_reward/numberOfEpisodes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nNLR16BLMUy"
      },
      "source": [
        "#### 1.2) Q Learning\n",
        "---\n",
        "\n",
        "Q Learning is a Reinforcement Learning technique used to determine the optimal policy in Markov Decision Processes. It uses a Q-Table which is a ***m x n*** lookup table, where ***m=states*** and ***n=actions***, used for the calculation of the maximum expected future reward for action at each state.\n",
        "\n",
        "Each score in the Q-Table will be the maximum expected future reward for that specific action at that specific state. To calculate these values, we use the Q-function. Under a given policy $\\pi$, the true value of an action ***a*** in a state ***s*** is \n",
        "\n",
        "![equation](https://miro.medium.com/max/1400/1*6IqzImIFK1oEiVWmlu1Esw.png)\n",
        "\n",
        "where $\\gamma \\in [0,1]$ is a discount factor that performs the immediate and later rewards importance trade-off.\n",
        "\n",
        "The Q-function can identify an optimal action-selection policy given any finite Markov decision process and return the expected rewards for an action taken in a given state. It is estimated using Q-learning which iteratively updates ***Q(state,action)*** using the Bellman Equation.\n",
        "\n",
        "To make sure that there is balance between exploring states and exploiting rewards, we will be using an epsilon greedy strategy \n",
        "\n",
        "![equation](https://miro.medium.com/max/1400/1*9StLEbor62FUDSoRwxyJrg.png)\n",
        "\n",
        "\n",
        "As Asterix is a game where an agent moves around stage lines in order to collect rewards and avoid being killed, we can classify it as a good environment for Reinforcement Learning as there are states, opportunities for the player to learn a control policy through actions, and scalar rewards. And we can utilise the Q-function as our game has episodes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82TcS7LGbjJp"
      },
      "source": [
        "#### 1.2) Deep Q-Learning\n",
        "\n",
        "As games become more complex, the efficiency of producing and updating a Q-table can become lower and lower, especially when dealing with bigger state space environments. To optimize Q-Learning, we can replace the Q-table with a Deep Q Neural Network (DQN).\n",
        "\n",
        "\n",
        "![equation](https://knowledge.dataiku.com/latest/_images/illustration-73.jpg)\n",
        "\n",
        "The DQN will have frames as input and a vector of Q-values for each possible action in the given state as output. The biggest Q-value of this vector is considered for the best action.\n",
        "\n",
        "The concept of a DQN can apply to and solve complex problems and environments. It is capable of taking actions that it has not seen before.\n",
        "\n",
        "To avoid the possibility of a big Q-table and memory issues, we will implement a DQN for Asterix, which can be more time consuming but also more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SkTwNPxuYjy"
      },
      "source": [
        "#### 1.3) Pre-processing\n",
        "\n",
        "---\n",
        "\n",
        "We are using a special wrapper for the purpose of environment preprocessing. \n",
        "\n",
        "This reshapes the image to .... and the agent will receive a grayscale observation as we have converted the observarions to grayscale. We also scale all observations to [0,1].\n",
        "\n",
        "Another common preprocessing step is the introductioon of frame skipping (Naddaf, 2010 at https://era.library.ualberta.ca/items/a661eb66-f2e0-4ed3-b501-b6cbcd1fdd9d), which is what restricts the agent's decision points by repeating some selected action for ***k*** consequitve frames, making the RL problem simpler and speeding up execution. \n",
        "\n",
        "This notebook follows an approach where the agent is designed with a richer observation space by combining past frames with most recent ones ask nown as frame stacking (Mnih et al., 2015 https://www.nature.com/articles/nature14236). We use 4 frames, but the algorithm might also be robust with different values such as 3 or 5. Due to time concerns, this notebook did not have the ability to experiment with such values.\n",
        "\n",
        "This actually reduces the degree of partial observability, making it possible for the agnet to detect the direction of motion in game objects. Due to this reduction in partial observability, the agent can detect the direction of motion of objects in the game.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stg_UVZsuYjz",
        "outputId": "66ddac83-00cb-4852-9c32-106ded138ba4"
      },
      "source": [
        "obs = env.reset()\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "def preprocess_observation(observation):\n",
        "    img = observation[23:153:2, ::2] # This becomes 90, 80, 3\n",
        "#     print(img.shape)\n",
        "#     print(img.shape)\n",
        "#     img = img.mean(axis=2) #to grayscale (values between 0 and 255)\n",
        "    img = rgb2gray(img)\n",
        "    img = img.astype(np.float32) # normalize from -128 to 127\n",
        "    return img.reshape(65, 80, 1)\n",
        "\n",
        "plt.imshow(obs)\n",
        "plt.show()\n",
        "print(preprocess_observation(obs)[0].dtype)\n",
        "plt.imshow(preprocess_observation(obs).reshape(65,80), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float32\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFqfohauYj0"
      },
      "source": [
        "1.4. Create a function `q_network` that (1) takes as input a variable of shape $(96,80,1)$, (2) creates a deep convolutional network with 3 convolutional hidden layers and two dense layers, and (3) returns the output layer and the trainable variables in a dictionary where the keys are the name of the variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWjyW1s5uYj0"
      },
      "source": [
        "# Import the Keras layer used for building our models\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D,\\\n",
        "MaxPool2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "INPUT_SHAPE = (65, 80, 1)\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "BATCH_SIZE = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkR88iGKuYj0"
      },
      "source": [
        "1.5. Create an agent `QLearningAgent` with two deep convolutional networks (`online` and `target`) to predict the action to be taken from the game's frames. The `online` network will be trained and replace the `target`network every $5000$ training steps.\n",
        "\n",
        "\n",
        "1.6. Create a method `get_action` using a parameter $\\epsilon$ for making random moves. This parameter will decrease from $1$ to $0.1$.\n",
        "\n",
        "1.7. Create a method `train` to update the weights of the `online` network using the Q-values of the `target` network. We will use a discount rate of $0.99$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tenbiwtv2Qe"
      },
      "source": [
        "read this to check architecture and fig\n",
        "\n",
        "https://www.nature.com/articles/nature14236\n",
        "\n",
        "ARCHITECTURE\n",
        "\n",
        "The first hidden layer convolves 32 filters of 8 × 8 with stride 4 with the input image and applies a rectifier nonlinearity31,32. The second hidden layer convolves 64 filters of 4 × 4 with stride 2, again followed by a rectifier nonlinearity. This is followed by a third convolutional layer that convolves 64 filters of 3 × 3 with stride 1 followed by a rectifier. The final two hidden layers are fully-connected and consist of 512 and 256 rectifier units respectively. The output layer is a fully-connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3cBQ8j0uYj1"
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "class QLearningAgent():\n",
        "    def __init__(self, env, lr=0.001):\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.loss_val = np.inf\n",
        "        self.action_size = NUM_ACTIONS\n",
        "#         tf.reset_default_graph()\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "        self.discount_rate = 0.99\n",
        "        self.lr =lr\n",
        "        self.checkpoint_path = \"./my_dqn.ckpt\"\n",
        "\n",
        "        # Create the two networks for predicting the actions\n",
        "        # The first model makes the predictions for Q-values \n",
        "        # which are used to make a action.\n",
        "        self.online = self.q_network()\n",
        "        \n",
        "        # Build a target model for the prediction of future\n",
        "        # rewards. The weights of a target model get updated \n",
        "        # every 5000 steps thus when the loss between the \n",
        "        # Q-values is calculated the target Q-value is stable.\n",
        "        self.target = self.q_network()\n",
        "\n",
        "        #The \"target\" DNN will take the values of the \"online\" DNN\n",
        "        self.update_target()\n",
        "\n",
        "        # Saving the session\n",
        "#         self.saver = tf.train.Saver()\n",
        "#         self.sess = tf.Session()\n",
        "#         if os.path.isfile(self.checkpoint_path + \".index\"):\n",
        "#             self.saver.restore(self.sess, self.checkpoint_path)\n",
        "#         else:\n",
        "#             self.sess.run(tf.global_variables_initializer())\n",
        "#             self.sess.run(self.copy_online_to_target)\n",
        "\n",
        "    def q_network(self, filters_1=32, filters_2=64, filters_3=64, units=256):\n",
        "        \"\"\"Define and return the CNN model architecture\n",
        "        \"\"\"\n",
        "        # Ensure reproducibility of the results\n",
        "        # by resetting the random seeds\n",
        "        # reset_random_seeds()\n",
        "        init = tf.keras.initializers.HeUniform()\n",
        "        # Build the model\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=8, padding=\"same\", strides=4, activation='relu', input_shape=INPUT_SHAPE, kernel_initializer=init))\n",
        "        model.add(Conv2D(64, kernel_size=4, padding=\"same\", strides=2, activation='relu', kernel_initializer=init))\n",
        "        model.add(Conv2D(64, kernel_size=3, padding=\"same\", strides=1, activation='relu', kernel_initializer=init))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu', kernel_initializer=init))\n",
        "        model.add(Dense(256, activation='relu', kernel_initializer=init))\n",
        "        model.add(Dense(NUM_ACTIONS, activation=\"linear\", kernel_initializer=init))\n",
        "        model.compile(loss=\"huber_loss\", optimizer=Adam(learning_rate=self.lr))\n",
        "        return model\n",
        "            \n",
        "    def update_target(self):\n",
        "        \"\"\"Update the target network with the online network weights\n",
        "        \"\"\"\n",
        "        # Get the online DQN weights\n",
        "        online_weights = self.online.get_weights()\n",
        "        \n",
        "        # Update the target DQN weights\n",
        "        self.target.set_weights(online_weights)\n",
        "        \n",
        "    #---- CHOSSING ACTION ----\n",
        "    def get_action(self, state, step):\n",
        "        epsilon = max(0.1, 1 - (0.9/2000000) * step)\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.randint(self.action_size) # random action\n",
        "        else:\n",
        "            q_values = self.online.predict(state.reshape(-1, *INPUT_SHAPE))\n",
        "            return np.argmax(q_values) # optimal action\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"Train the online model and update the loss value\n",
        "        \"\"\"\n",
        "        learning_rate=0.3\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        \n",
        "        samples = random.sample(self.memory, BATCH_SIZE)\n",
        "        \n",
        "        curr_states = np.array([sample[0] for sample in samples])\n",
        "        states_q_vals = self.online.predict(curr_states)\n",
        "        \n",
        "        next_states = np.array([sample[0] for sample in samples])\n",
        "        targets_q_vals = self.target.predict(next_states)\n",
        "\n",
        "        X, Y = [], []\n",
        "        \n",
        "        for index, (curr_state, action, reward, next_state, done) in enumerate(samples):\n",
        "            if done:\n",
        "                future_q_val = reward\n",
        "            else:\n",
        "                future_q_val = reward + np.amax(targets_q_vals[index]) * self.discount_rate\n",
        "            \n",
        "            # Get the current Q value\n",
        "            curr_q_val = states_q_vals[index]\n",
        "#             curr_q_val[action] = future_q_val\n",
        "            curr_q_val[action] = (1 - learning_rate) * curr_q_val[action] + learning_rate * future_q_val\n",
        "            X.append(curr_state)\n",
        "            Y.append(curr_q_val)\n",
        "            \n",
        "        hist = self.online.fit(np.array(X), np.array(Y), batch_size=BATCH_SIZE, epochs=1, verbose=0, shuffle=True)\n",
        "        self.loss_val = hist.history['loss'][0]\n",
        "    \n",
        "    def save_to_memory(self, curr_state, action, reward, next_state, done):\n",
        "        self.memory.append([curr_state, action, reward, next_state, done])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuYU9kuJuYj1"
      },
      "source": [
        "1.8. Train your network for $1,000,000$ training steps. Since the training process can take __a lot of time__, save your models every $1000$ training steps. You can choose to only train your model every $4$ frames instead of every frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbZdIHafuYj2",
        "scrolled": true,
        "outputId": "7fe403c2-4fed-4a5d-b012-5fd2dc135b10"
      },
      "source": [
        "import time\n",
        "agent = QLearningAgent(env)  \n",
        "ep_rewards = []\n",
        "total_reward = 0\n",
        "n_steps = 1000000  # total number of training steps\n",
        "copy_steps = 5000\n",
        "save_steps = 1000 \n",
        "\n",
        "# Train the model after 4 actions\n",
        "train_online_steps = 4\n",
        "\n",
        "# Train the model after 4 actions\n",
        "# Considered as a hyperparameter\n",
        "update_target_steps = 100\n",
        "\n",
        "done=True\n",
        "for step in range(n_steps):\n",
        "    \n",
        "    \n",
        "\n",
        "    if done: # game over, start again\n",
        "        obs = env.reset()\n",
        "        state = preprocess_observation(obs)\n",
        "        if step != 0:\n",
        "            ep_rewards.append(total_reward)\n",
        "            print('\\n Reward this run: {} after n steps = {} with an average reward = {}'.format(total_reward, step, mean(ep_rewards)))\n",
        "            total_reward = 0\n",
        "    \n",
        "    \n",
        "    print(\"\\r\\tAction step: {}/{} ({:.1f}%)\\tLoss: {:5f}\".format(\n",
        "        step,\n",
        "        n_steps,\n",
        "        step * 100 / n_steps, \n",
        "        agent.loss_val), end=\"\")\n",
        "        \n",
        "\n",
        "    total_perc = int(step * 100 / n_steps)\n",
        "\n",
        "    # Get a exploration/exploitation action depending on the\n",
        "    # current epsilon value\n",
        "#     q_values = agent.online.predict(state)\n",
        "    action = agent.get_action(state, step)\n",
        "\n",
        "    # Online DQN plays\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "    next_state = preprocess_observation(next_obs)\n",
        "    agent.save_to_memory(state, action, reward, next_state, done)\n",
        "    \n",
        "    if step % train_online_steps == 0:\n",
        "        agent.train()\n",
        "\n",
        "    env.render()\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "    # Regularly copy the online DQN to the target DQN\n",
        "    if step % update_target_steps == 0:\n",
        "        agent.update_target()\n",
        "#     # And save regularly\n",
        "#     if step % save_steps == 0:\n",
        "#         agent.saver.save(agent.sess, agent.checkpoint_path)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tAction step: 4/1000000 (0.0%)\tLoss:   inf"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\uni\\masters\\applied_ai\\aai\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tAction step: 291/1000000 (0.0%)\tLoss: 0.000195\n",
            " Reward this run: 100.0 after n steps = 292 with an average reward = 100.0\n",
            "\tAction step: 683/1000000 (0.1%)\tLoss: 0.001397\n",
            " Reward this run: 350.0 after n steps = 684 with an average reward = 225.0\n",
            "\tAction step: 990/1000000 (0.1%)\tLoss: 0.000313\n",
            " Reward this run: 200.0 after n steps = 991 with an average reward = 216.66666666666666\n",
            "\tAction step: 1311/1000000 (0.1%)\tLoss: 0.000038\n",
            " Reward this run: 200.0 after n steps = 1312 with an average reward = 212.5\n",
            "\tAction step: 1558/1000000 (0.2%)\tLoss: 0.007683\n",
            " Reward this run: 100.0 after n steps = 1559 with an average reward = 190.0\n",
            "\tAction step: 1938/1000000 (0.2%)\tLoss: 0.019895\n",
            " Reward this run: 350.0 after n steps = 1939 with an average reward = 216.66666666666666\n",
            "\tAction step: 2228/1000000 (0.2%)\tLoss: 0.000538\n",
            " Reward this run: 150.0 after n steps = 2229 with an average reward = 207.14285714285714\n",
            "\tAction step: 2582/1000000 (0.3%)\tLoss: 0.003307\n",
            " Reward this run: 300.0 after n steps = 2583 with an average reward = 218.75\n",
            "\tAction step: 3009/1000000 (0.3%)\tLoss: 0.000074\n",
            " Reward this run: 400.0 after n steps = 3010 with an average reward = 238.88888888888889\n",
            "\tAction step: 3340/1000000 (0.3%)\tLoss: 0.010855\n",
            " Reward this run: 350.0 after n steps = 3341 with an average reward = 250.0\n",
            "\tAction step: 3615/1000000 (0.4%)\tLoss: 0.000116\n",
            " Reward this run: 150.0 after n steps = 3616 with an average reward = 240.9090909090909\n",
            "\tAction step: 4029/1000000 (0.4%)\tLoss: 0.003542\n",
            " Reward this run: 250.0 after n steps = 4030 with an average reward = 241.66666666666666\n",
            "\tAction step: 4325/1000000 (0.4%)\tLoss: 0.046686\n",
            " Reward this run: 200.0 after n steps = 4326 with an average reward = 238.46153846153845\n",
            "\tAction step: 4620/1000000 (0.5%)\tLoss: 0.010375\n",
            " Reward this run: 100.0 after n steps = 4621 with an average reward = 228.57142857142858\n",
            "\tAction step: 5014/1000000 (0.5%)\tLoss: 0.005317\n",
            " Reward this run: 250.0 after n steps = 5015 with an average reward = 230.0\n",
            "\tAction step: 5528/1000000 (0.6%)\tLoss: 0.004171\n",
            " Reward this run: 500.0 after n steps = 5529 with an average reward = 246.875\n",
            "\tAction step: 5806/1000000 (0.6%)\tLoss: 0.060541\n",
            " Reward this run: 50.0 after n steps = 5807 with an average reward = 235.2941176470588\n",
            "\tAction step: 6235/1000000 (0.6%)\tLoss: 0.105096\n",
            " Reward this run: 400.0 after n steps = 6236 with an average reward = 244.44444444444446\n",
            "\tAction step: 6531/1000000 (0.7%)\tLoss: 0.156928\n",
            " Reward this run: 200.0 after n steps = 6532 with an average reward = 242.10526315789474\n",
            "\tAction step: 6908/1000000 (0.7%)\tLoss: 0.026102\n",
            " Reward this run: 350.0 after n steps = 6909 with an average reward = 247.5\n",
            "\tAction step: 7179/1000000 (0.7%)\tLoss: 0.046035\n",
            " Reward this run: 200.0 after n steps = 7180 with an average reward = 245.23809523809524\n",
            "\tAction step: 7489/1000000 (0.7%)\tLoss: 0.230552\n",
            " Reward this run: 200.0 after n steps = 7490 with an average reward = 243.1818181818182\n",
            "\tAction step: 7787/1000000 (0.8%)\tLoss: 0.159022\n",
            " Reward this run: 50.0 after n steps = 7788 with an average reward = 234.7826086956522\n",
            "\tAction step: 8087/1000000 (0.8%)\tLoss: 0.102744\n",
            " Reward this run: 300.0 after n steps = 8088 with an average reward = 237.5\n",
            "\tAction step: 8432/1000000 (0.8%)\tLoss: 0.287747\n",
            " Reward this run: 400.0 after n steps = 8433 with an average reward = 244.0\n",
            "\tAction step: 8708/1000000 (0.9%)\tLoss: 0.099554\n",
            " Reward this run: 100.0 after n steps = 8709 with an average reward = 238.46153846153845\n",
            "\tAction step: 9080/1000000 (0.9%)\tLoss: 0.012163\n",
            " Reward this run: 350.0 after n steps = 9081 with an average reward = 242.59259259259258\n",
            "\tAction step: 9405/1000000 (0.9%)\tLoss: 0.166005\n",
            " Reward this run: 200.0 after n steps = 9406 with an average reward = 241.07142857142858\n",
            "\tAction step: 9677/1000000 (1.0%)\tLoss: 0.357685"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ybzGC0quYj2"
      },
      "source": [
        "1.8. Print the evolution of the total number of rewards w.r.t. the episodes.\n",
        "\n",
        "1.9. Analyse the behaviour of the saved agents, can you notice any emerging strategies?\n",
        "\n",
        "https://www.youtube.com/watch?v=OxOqLQ8Ed_k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TCIGwXPuYj2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8vTmSsLuYj2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGv61B1EunHO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tvvxU6CunPl"
      },
      "source": [
        "https://github.com/mila-iqia/atari-representation-learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY6nofTRunX-"
      },
      "source": [
        "\n",
        "#### Part 2\n",
        "---\n",
        "\n",
        "Reinforcement Learning from the Screen Frames\n",
        "\n",
        "###2.1) RAM Description and Processing techniques\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9kBgRa5uyNt"
      },
      "source": [
        "#Using RAM, each observation is an array of length 128.\n",
        "# it is porbably a better represnetation\n",
        "\n",
        "# RAM is helping us avoid the task of learning a representation of the game\n",
        "# lets you move directly to learning a policy based on game state\n",
        "\n",
        "# this is an image based environment\n",
        "env = gym.make('Breakout-v0')\n",
        "env.reset()\n",
        "\n",
        "# put in the 0 action \n",
        "observation_image, reward, done, info = env.step(0)\n",
        "\n",
        "# get the ram observation with the code below\n",
        "observation_ram = env.unwrapped._get_ram()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}