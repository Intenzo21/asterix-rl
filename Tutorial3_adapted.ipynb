{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University of Aberdeen\n",
    "\n",
    "## Applied AI (CS5079)\n",
    "\n",
    "### Tutorial (Day 3) - Reinforcement Learning with OpenAI Gym\n",
    "\n",
    "---\n",
    "\n",
    "The practical is inspired from the code on: https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "On successful completion of this component a student will have demonstrated competence in creating and training an agent to play an Atari game using the screen frames as input\n",
    "\n",
    "### In this tutorial, we will use the following libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\program files\\python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "c:\\program files\\python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# For deep neural networks\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# For data representation\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# For handling files\n",
    "import os\n",
    "\n",
    "# For plotting graphs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# OpenAI Gym\n",
    "import gym\n",
    "env = gym.make(\"Asterix-v0\")\n",
    "# I have installed pyglet-1.5.11 for it work with BigSur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari Games\n",
    "\n",
    "In this tutorial, we will create an agent to play the Boxing Atari game. In this game, two boxers, one white and one black, compete against each other. When close enough, a boxer can hit his opponent with a punch which causes his opponent to reel back slightly. A match is completed either when one player lands 100 punches or two minutes have elapsed. In the case of a decision, the player with the most landed punches is the winner. Ties are possible. \n",
    "\n",
    "![Activision's Boxing Cartridge](img/Boxing.png)\n",
    "\n",
    "In this tutorial, we will use the pixels as inputs. The OpenAI Gym environment is `Boxing-v0`. \n",
    "\n",
    "### Naive Agent\n",
    "1.1. For the boxing game, describe the observations, the action space, the reward, the environment’s info dictionary and the episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(210, 160, 3)\n",
      "Action space: Discrete(9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space is an RGB picture of size 210x160 whereas the action space is composed of the 18 possible moves from an ATARI controller. The info dictionary contains `ale.lives()` which refers to the number of lives left (but here there is no finite number of lives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Create a simple agent that performs random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent():\n",
    "    def __init__(self, env):\n",
    "        self.action_size = env.action_space.n\n",
    "        \n",
    "    def get_action(self, observation):\n",
    "        return random.choice(range(self.action_size))\n",
    "    \n",
    "total_reward=0\n",
    "agent = RandomAgent(env)\n",
    "numberOfEpisodes = 10\n",
    "# for steps in range(numberOfEpisodes):\n",
    "#     current_obs = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = agent.get_action(current_obs)\n",
    "#         next_obs, reward, done, info = env.step(action)\n",
    "#         total_reward += reward\n",
    "#         env.render()\n",
    "print(\"Average reward: {}\".format(total_reward/numberOfEpisodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning Agent\n",
    "\n",
    "1.3. Implement a pre-processing function to convert the $210\\times160$ RGB frames to $96\\times80$ greyscale frames. Then, change the type of the matrix to `int8`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATZElEQVR4nO3de7RcZXnH8e8zcy4kIXdCiEkkCUQjRIiIkVVDiigIqRrRrhikii3LaAtV8dIVdNXS+o+3aOvC4opLFsEKRBEKpaCkoEZXRUkwJOESSDCRc5p7IAlJPJeZp3/sfSZzkjM5J+/smT2X32etWWfedy773WR+7D173v1sc3dE5ORk0h6ASD1ScEQCKDgiARQckQAKjkgABUckQMWCY2ZXmNkmM9tsZksrtRyRNFglfscxsyzwPHAZ0AE8AVzt7s8kvjCRFFRqizMX2OzuL7p7N3A3sLBCyxKpupYKve9k4KWidgfw1lJPNjNNX5BatMfdJwz0QKWCMygzWwIsSWv5IkOwrdQDlQpOJzC1qD0l7itw9+XActAWR+pPpb7jPAHMNLPpZtYGLAYeqNCyRKquIlscd+81sxuAnwFZ4DZ3f7oSyxJJQ0UOR5/0ILSrJrVprbtfONADmjkgEkDBEQmg4IgESO13HClfxk7hrHGfOuFztuz7V/LeVaURNQ8Fp460Z89g8qhFhbZZC6PaZ5/wNdPHXo97b6HdeWAlXbmdFRtjs1BwatiEkW9i8rhLCu3WzChGnXL+Sb3HmNHz+rXHjRlDb/5god2x7zH2HHyqrHE2IwWnhkwd905GDz+r0B49bAbjR554i3KyRgzrP2VweNtEzhh9UaH9yuEX6Nj3WKLLbEQKToreeOYiTmkdXWhPGvNnjBz22qqOYeSItwBvKbQPHNnGpHFHw3uk+xU2/vHHVR1TPVBwqui8Mz9IdKpSZM6MaxjWNjbFER1v5IhZTD5tVqF9uGsf2UxroZ33HBu2rUxjaDVFwakY44KzP9Cv5+JzbiRTFJx6MKx9Ipeef2OhnfdeWltz/Z7z5OZ7qj2s1Ck4CWnJnsLrp7y90DYz3nPRP6c4oso5dr26ew9TPHVrU8dj9OYa+xC4ghMokxlJa+vR7wLD2sfyljd8qd9ztu05UO1hpeLCWf/Yr9358hGOdL1SaHf3bMGLjuQ1Ak3yHKJM9jSy2fGFdmvrTEaNvibFEdWPA/t/QE/PlkI7l9tDPrc3xRENWclJngpOCW1tk8hkhxfaw0e8k+HD35HiiBrH4UOrOHz46CHvfO4w3d3bUxxRSQrOYFrbzsDs6NS9KWd+hhGnJvsbigzs0MENdPzxW4W2e46e7pqY3aDgHKu9vX8NhrNn3UJL65hqD0MG0NOzjy3PfbJfX1fX7jSGouC0Fv3QCDDv0of7bWGkdrnn+PVjC/r19fTsr8aikw+OmU0F7gAmAg4sd/d/M7ObgY8Bff+L+IK7PzTIeyUcHKOlZVihlcm0Me/Snya7CEnVrx57F57vKbR7e48QfQwTVZHgTAImufuTZjYSWAu8D1gEvOru3ziJ9ypzjTNkMkd/WBw5agaXvfu+8t5S6sojDy7k1QNbC+18vpcEglQyOMG/47j7dmB7fP+gmT1LVIiw6qZOu5K5b/tKvz73XIlnSyO67C/u7df+7a8/T8e2yu1lJPIdx8ymAauB2cBngI8CB4A1wGfd/eVBXp/+Fy2R41WuWIeZnQr8BPi0ux8AbgXOAuYQbZGWlXjdEjNbY2Zryh2DSLWVtcUxs1bgQeBn7v7NAR6fBjzo7if8QURbHKlRyW9xzMyA7wPPFocmPmjQ5ypgY+gyRGpVOZM83wZ8GNhgZuvivi8AV5vZHKJDGluBj5exDJGa1DQ/gIoEUCVPkSQpOCIBFByRAAqOSAAFRySAgiMSQMERCaDgiARoivJQZlnOPf+Tgz9R6sbTT3071VNHmiM4mRZmzf5Y2sOQBD2z/jsKTqV5PseWTXelPQxJUNonKmqumkhpmqsmkiQFRySAgiMSQMERCaDgiAQo+3C0mW0FDgI5oNfdLzSzccBKYBrR6dOLBisRJVJPktrivN3d5xQdulsKPOruM4FH47ZIw6jUrtpCYEV8fwVRaVyRhpFEcBx4xMzWmtmSuG9iXCIXYAdRYXaRhpHElJt57t5pZqcDq8zsueIH3d0HmhkQh2zJsf0i9aDsLY67d8Z/dwH3AXOBnX2FCeO/uwZ43XJ3v7DUlAaRWlZWcMxsRHyJD8xsBHA5UeXOB4Br46ddC9xfznJEak25u2oTgfuiari0AHe6+0/N7AngR2Z2HbCN6Jo5Ig2jKWZHZzJtLPzg7yq5CKmy+1fOJZ/vrvRikr+wVF0xI5ttS3sU0kCaYosD0NY+ptKLkCrq7nqlGotp8i0OVfsPLU1CkzxFAig4IgEUHJEACo5IAAVHJICCIxJAwREJoOCIBFBwRAIoOCIBFByRAAqOSIAmmeRpjBv/xrQHIQnat3cDUZ2YdDRFcDLZNi698u60hyEJuvfOOdU4ka2k4OCY2euJqnX2mQF8CRgDfAzYHfd/wd0fCl1OItw59GpHqkOQpKV7HlkiJ7KZWRboBN4K/DXwqrt/4yRen/7ZdCLHq/iFpd4BbHH3bQm9n0hNSyo4i4Hii2zeYGbrzew2Mxub0DJEakbZwTGzNuC9wI/jrluBs4A5wHZgWYnXLTGzNWa2ptwxiFRb2d9xzGwhcL27Xz7AY9OAB9199iDvoe84Uosq+h3naop20/pK38auIqrsKdJQyvodJy57exnw8aLur5nZHKLjhVuPeUykITRNXTWRABU/HC3SVBQckQAKjkgABUckgIIjEkDBEQnQHOfjZFqZf9ntaQ9DEvTLVdfi+d7Ult8UwcEynDbhTWmPQhJkZFI9I6cpgpPP9/Cb1Z9OexiSoHyKWxvQzAGRE9HMAZEkKTgiARQckQAKjkgABUckgIIjEkDBEQkwpODEZZ52mdnGor5xZrbKzF6I/46N+83Mvm1mm+MSURdUavAiaRnqFud24Ipj+pYCj7r7TODRuA1wJTAzvi0hKhcl0lCGFBx3Xw3sO6Z7IbAivr8CeF9R/x0eeRwYc0zlG5G6V853nInuvj2+vwOYGN+fDLxU9LyOuK8fFSSUepbIJE9395Odb+buy4HloLlqUn/KCc5OM5vk7tvjXbFdcX8nMLXoeVPivlSZNcVE8Kbhnu7s6HI+TQ8A1wJfif/eX9R/g5ndTXTZj/1Fu3SpyGTbef/Vv09zCJKwuriwlJndBVwCnGZmHcA/EQXmR2Z2HbANWBQ//SFgAbAZOEx0vZzU1cLpE9I4dD6OSGk6H0ckSQqOSAAFRySAgiMSQMERCaDgiARQcEQCKDgiARQckQAKjkgABUckgIIjEkDBEQnQFGd3mWWZ+YaPpD0MSdALz67APZ/a8psjOJkWzrvg82kPQxK0+bkf4l7jJ7LVO/ccL219KO1hSILS3NqATmQTOZHwE9lKVPH8upk9F1fqvM/MxsT908zsiJmti2/fTWwVRGrIUI6q3c7xVTxXAbPd/TzgeeCmose2uPuc+PaJZIYpUlsGDc5AVTzd/RE/Wp/ncaISUCJNI4nfcf4GeLioPd3Mfm9mvzSzi0u9SJU8pa65+6A3YBqwcYD+LwL3cfQgQzswPr7/ZqJSuKOG8P6um241eFtT6jMbvMUxs48C7wau8b5Pv3uXu++N768FtgCvC12GSK0KCo6ZXQH8A/Bedz9c1D/BzLLx/RlEl/p4MYmBitSSQX8ALVHF8yai3bJVZgbweHwEbT7wL2bWA+SBT7j7sZcHEal7+gFUpDRV8hRJkoIjEkDBEQmg4IgEaIrTCjKZNt7zl79KexiSoP+6Zx75fE9qy2+K4GBGa9vItEchibJ0l94sh6OHj9AV4xvJ4UNVuTpmycPRzbHFoWr/oaVJ6OCASAAFRySAgiMSQMERCaDgiARQcEQCKDgiARQckQAKjkiA0EqeN5tZZ1HFzgVFj91kZpvNbJOZvatSAxdJU2glT4BvFVXsfAjAzM4BFgPnxq/5977iHSKNZNC5au6+2symDfH9FgJ3u3sX8Acz2wzMBX4TPsTBZW0E7S0TKrkIqTNdvbvJ+aGKvX85kzxvMLOPAGuAz7r7y8BkopK4fTrivuOY2RJgSRnLLzi1bSaTRy0utDPWSnvL6Um8tdSJrt6d5AtVmaHjwJ0c6FpfseWFBudW4MtE1Q6/DCwjKoU7ZO6+HFgO5Z9WsL9rHft3ryu027On87rxRXXgzRjWqi1SIznSsxuKTol5fu9X6c7trtryg4Lj7jv77pvZ94AH42YnMLXoqVPivqrqyu1iw64bC+2MtXLZ7Nv7PaetZTRxTTipce5Od+/+fn1rNn6u3xam2oKCY2aT3L3vBJergL4jbg8Ad5rZN4HXEFXy/F3ZoyxT3nv42YZr+vVdcd7KfsFpzQ5XkGqEu9OTO1zUzh/375e20Eqel5jZHKJdta3AxwHc/Wkz+xHwDNALXO/uuYqMvEw/Xf/Bfu0Pzb+H4e3jC+1sphUz/cxVDe55ckX1Aw517eau1YtSHNHgmubU6ZO14M3LmDJ+bqFtZgpSQtzzFH/uXtrzOA8/WZMXNy556rSCM0SXnHc989/4t/36tGs3NMd+xn6x/hZWb6iLq1wqOEk7+zXz+NDb6+IfP3X/8dgSXtz+v2kPI4SCU2mZ7DhOn/idtIdRE3bt+Dvy+ZfTHkYSFJzqyPS7f8ZrfpDaSKppx//9FdFxoj75tIaSNAUnHf0PWp57/r1YpjWlsSQjn+/mmac+cExver+nVJiCUwsymWH92mfPuoX2UwackVQzuv7Uwebn/r5fXz5/JKXRVJ2CU4sy2VOxot27WbNvYsLEP09xRLBrx8/Z9PRXC20nRz5XucmSNU7BqQctraPIZtoL7UlT3sOMmYnMgy1py/PfZUfnfxfauXwXvT0HKrrMOqLg1KOWlpG0to0ttMeMm8VF85aV9Z6P/+pGXnn5+UK7p3sfvb2vlvWeDUzBaQTZ7DBGjppWaLe0juCSy+844Wt+8ciH6e05Ou/r4IE/kMv9qVJDbDQKTiMyyzJ+wpwTPmfv7nXU6HTBeqCrFTQi9xx7dq1NexhNSbMWRQIoOCIBFByRAAqOSIDQgoQri4oRbjWzdXH/NDM7UvSY5t1LQxrKUbXbgVuAwg8G7l4479jMlgHFlRS2uPuchMYnUpPKKkho0SmQi4BLEx6XSE0r9zvOxcBOd3+hqG+6mf3ezH5pZheX+f4iNancH0CvBu4qam8HXuvue83szcB/mtm57n7crMEkK3mKVFvwFsfMWoD3Ayv7+ty9y933xvfXAluA1w30endf7u4XlprSIFLLytlVeyfwnLt39HWY2YS+qxOY2QyigoQvljdEkdozlMPRdxFdbeD1ZtZhZtfFDy2m/24awHxgfXx4+h7gE+6+L8HxitQEzY4WKa3k7GjNHBAJoOCIBFBwRAIoOCIBFByRAAqOSAAFRySAgiMSQMERCaDgiARQcEQCKDgiARQckQAKjkgABUckgIIjEkDBEQkwlFOnp5rZz83sGTN72sw+FfePM7NVZvZC/Hds3G9m9m0z22xm683sgkqvhEi1DWWL0wt81t3PAS4Crjezc4ClwKPuPhN4NG4DXElUpGMmUfmnWxMftUjKBg2Ou2939yfj+weBZ4HJwEJgRfy0FcD74vsLgTs88jgwxswmJT1wkTSd1HecuBTum4DfAhPdfXv80A5gYnx/MvBS0cs64j6RhjHkSp5mdirwE+DT7n4gKhsdcXc/2Uo1quQp9WxIWxwzayUKzQ/d/d64e2ffLlj8d1fc3wlMLXr5lLivH1XylHo2lKNqBnwfeNbdv1n00APAtfH9a4H7i/o/Eh9duwjYX7RLJ9IY3P2EN2Ae4MB6YF18WwCMJzqa9gLwP8C4+PkGfIeobvQG4MIhLMN1060Gb2tKfWZVyVOkNFXyFEmSgiMSQMERCaDgiARQcEQClHsN0KTsAQ7FfxvFaTTO+jTSusDQ1+fMUg/UxOFoADNb00izCBppfRppXSCZ9dGumkgABUckQC0FZ3naA0hYI61PI60LJLA+NfMdR6Se1NIWR6RupB4cM7vCzDbFxT2WDv6K2mNmW81sg5mtM7M1cd+AxUxqkZndZma7zGxjUV/dFmMpsT43m1ln/G+0zswWFD12U7w+m8zsXUNayGBT/it5A7JEpx/MANqAp4Bz0hxT4HpsBU47pu9rwNL4/lLgq2mP8wTjnw9cAGwcbPxEp5Q8THT6yEXAb9Me/xDX52bgcwM895z4c9cOTI8/j9nBlpH2FmcusNndX3T3buBuomIfjWAhAxczqTnuvhrYd0x3qfEvpMaLsZRYn1IWAne7e5e7/wHYTPS5PKG0g9MohT0ceMTM1sa1FKB0MZN60YjFWG6Idy9vK9p1DlqftIPTKOa5+wVENeWuN7P5xQ96tE9Qt4cv6338sVuBs4A5wHZgWTlvlnZwhlTYo9a5e2f8dxdwH9GmvlQxk3pRVjGWWuPuO9095+554Hsc3R0LWp+0g/MEMNPMpptZG7CYqNhH3TCzEWY2su8+cDmwkdLFTOpFQxVjOeZ72FVE/0YQrc9iM2s3s+lEFWh/N+gb1sARkAXA80RHM76Y9ngCxj+D6KjMU8DTfetAiWImtXgD7iLafekh2se/rtT4CSjGUiPr84N4vOvjsEwqev4X4/XZBFw5lGVo5oBIgLR31UTqkoIjEkDBEQmg4IgEUHBEAig4IgEUHJEACo5IgP8Hbo1T3QWDuOQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def preprocess_observation(observation):\n",
    "    img = observation[1:180:2, ::2] # This becomes 96, 80, 3\n",
    "#     print(img.shape)\n",
    "#     img = img.mean(axis=2) #to grayscale (values between 0 and 255)\n",
    "    img = rgb2gray(img)\n",
    "    img = img.astype(np.float32) # normalize from -128 to 127\n",
    "    return img.reshape(-1, 90, 80, 1)\n",
    "\n",
    "plt.imshow(obs)\n",
    "plt.show()\n",
    "print(preprocess_observation(obs)[0].dtype)\n",
    "plt.imshow(preprocess_observation(obs).reshape(90,80), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. Create a function `q_network` that (1) takes as input a variable of shape $(96,80,1)$, (2) creates a deep convolutional network with 3 convolutional hidden layers and two dense layers, and (3) returns the output layer and the trainable variables in a dictionary where the keys are the name of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Keras layer used for building our models\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D,\\\n",
    "MaxPool2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "INPUT_SHAPE = (90, 80, 1)\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5. Create an agent `QLearningAgent` with two deep convolutional networks (`online` and `target`) to predict the action to be taken from the game's frames. The `online` network will be trained and replace the `target`network every $5000$ training steps.\n",
    "\n",
    "\n",
    "1.6. Create a method `get_action` using a parameter $\\epsilon$ for making random moves. This parameter will decrease from $1$ to $0.1$.\n",
    "\n",
    "1.7. Create a method `train` to update the weights of the `online` network using the Q-values of the `target` network. We will use a discount rate of $0.99$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class QLearningAgent():\n",
    "    def __init__(self, env):\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.loss_val = np.inf\n",
    "        self.action_size = NUM_ACTIONS\n",
    "        tf.reset_default_graph()\n",
    "        tf.disable_eager_execution()\n",
    "        self.discount_rate = 0.99\n",
    "        self.checkpoint_path = \"./my_dqn.ckpt\"\n",
    "\n",
    "        # Create the two networks for predicting the actions\n",
    "        # The first model makes the predictions for Q-values \n",
    "        # which are used to make a action.\n",
    "        self.online = self.q_network()\n",
    "        \n",
    "        # Build a target model for the prediction of future\n",
    "        # rewards. The weights of a target model get updated \n",
    "        # every 5000 steps thus when the loss between the \n",
    "        # Q-values is calculated the target Q-value is stable.\n",
    "        self.target = self.q_network()\n",
    "\n",
    "        #The \"target\" DNN will take the values of the \"online\" DNN\n",
    "        self.update_target()\n",
    "\n",
    "        # Saving the session\n",
    "#         self.saver = tf.train.Saver()\n",
    "#         self.sess = tf.Session()\n",
    "#         if os.path.isfile(self.checkpoint_path + \".index\"):\n",
    "#             self.saver.restore(self.sess, self.checkpoint_path)\n",
    "#         else:\n",
    "#             self.sess.run(tf.global_variables_initializer())\n",
    "#             self.sess.run(self.copy_online_to_target)\n",
    "\n",
    "    def q_network(self, filters_1=32, filters_2=64, filters_3=64, units=256):\n",
    "        \"\"\"Define and return the CNN model architecture\n",
    "        \"\"\"\n",
    "        # Ensure reproducibility of the results\n",
    "        # by resetting the random seeds\n",
    "        # reset_random_seeds()\n",
    "\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=8, padding=\"same\", strides=4, activation='relu', input_shape=INPUT_SHAPE))\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\", strides=2, activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\", strides=1, activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(NUM_ACTIONS, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "            \n",
    "    def update_target(self):\n",
    "        \"\"\"Update the target network with the online network weights\n",
    "        \"\"\"\n",
    "        # Get the online DQN weights\n",
    "        online_weights = self.online.get_weights()\n",
    "        \n",
    "        # Update the target DQN weights\n",
    "        self.target.set_weights(online_weights)\n",
    "        \n",
    "    #---- CHOSSING ACTION ----\n",
    "    def get_action(self, state, step):\n",
    "        epsilon = max(0.1, 1 - (0.9/2000000) * step)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.action_size) # random action\n",
    "        else:\n",
    "            q_values = self.online.predict(state)\n",
    "            return np.argmax(q_values) # optimal action\n",
    "    \n",
    "    def train(self, state, action, reward, new_state, done):\n",
    "        \"\"\"Train the online model and update the loss value\n",
    "        \"\"\"\n",
    "#         print()\n",
    "#         print(state.shape, type(state))\n",
    "#         input()\n",
    "#         if len(self.memory) < BATCH_SIZE:\n",
    "#             return\n",
    "#         samples = random.sample(self.memory, BATCH_SIZE)\n",
    "#         states = np.array([state.reshape(90,80,1) for state, *other in samples])\n",
    "#         targets = self.target.predict(states)\n",
    "#         print()\n",
    "#         print(targets, states.shape)\n",
    "#         input()\n",
    "        \n",
    "        target = self.target.predict(state)\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            Q_future = np.amax(self.target.predict(new_state)[0])\n",
    "            target[0][action] = reward + Q_future * self.discount_rate\n",
    "#             targets.append(target)\n",
    "        hist = self.online.fit(state, target, epochs=1, verbose=0)\n",
    "        self.loss_val = hist.history['loss'][0]\n",
    "    \n",
    "    def save_to_memory(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8. Train your network for $1,000,000$ training steps. Since the training process can take __a lot of time__, save your models every $1000$ training steps. You can choose to only train your model every $4$ frames instead of every frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\tAction step: 0/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 1/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 2/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 3/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 4/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 5/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 6/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 7/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 8/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 9/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 10/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 11/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 12/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 13/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 14/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 15/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 16/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 17/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 18/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 19/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 20/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 21/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 22/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 23/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 24/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 25/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 26/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 27/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 28/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 29/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 30/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 31/1000000 (0.0%)\tLoss:   inf\r",
      "\tAction step: 32/1000000 (0.0%)\tLoss:   inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 3.35909612e-03  6.93080062e-03 -1.39201060e-04 -1.56468935e-02\n",
      "   5.28904423e-03 -1.19875930e-02 -8.41046311e-03 -4.68712859e-03\n",
      "   1.39762852e-02]\n",
      " [ 4.65540122e-03  1.29469056e-02  2.70340126e-04 -2.30806805e-02\n",
      "   1.93149084e-04 -6.37060916e-03  3.67169734e-04 -7.82597251e-03\n",
      "   2.32247785e-02]\n",
      " [-8.84171575e-04  1.53806657e-02  2.23951414e-04 -1.33261085e-02\n",
      "   8.32868274e-04 -1.13604665e-02 -1.08824600e-03 -5.92892803e-03\n",
      "   1.90251172e-02]\n",
      " [ 2.54053809e-03  1.07753901e-02  3.96577362e-03 -1.71638057e-02\n",
      "  -5.58109023e-05 -1.12543004e-02 -6.31460454e-03 -8.53031315e-03\n",
      "   1.69306416e-02]\n",
      " [ 3.90016474e-04  7.27119297e-03  5.24228998e-03 -1.99069045e-02\n",
      "  -2.38533947e-03 -1.33385845e-02 -8.19846429e-03 -1.73275883e-03\n",
      "   1.10662393e-02]\n",
      " [ 5.93808480e-04  8.85098800e-03  4.04955447e-03 -2.36082654e-02\n",
      "   5.18029230e-03 -9.50827263e-03  1.13156624e-03 -8.76346976e-03\n",
      "   1.21871512e-02]\n",
      " [-4.28344589e-03  1.21870395e-02 -1.04781357e-03 -1.66366100e-02\n",
      "  -1.69711863e-03 -7.59869395e-03 -4.40911762e-03 -2.05715373e-03\n",
      "   1.33706778e-02]\n",
      " [-5.99962613e-03  1.05210217e-02  2.16458831e-03 -2.15297490e-02\n",
      "   8.95438530e-03 -5.15438989e-03 -5.13677392e-03  1.60297798e-03\n",
      "   5.13632037e-03]\n",
      " [-3.75775062e-03  1.04771536e-02  2.34295381e-04 -2.03002840e-02\n",
      "   3.68824438e-03 -9.76163521e-03 -1.89027935e-03 -5.38535323e-03\n",
      "   1.26964506e-02]\n",
      " [-2.90665310e-03  7.53381616e-03  2.87176156e-03 -2.32296865e-02\n",
      "   1.77988363e-03 -1.60631668e-02 -3.37446434e-03 -8.55679810e-03\n",
      "   1.38908848e-02]\n",
      " [-7.10584922e-03  6.97477628e-03  7.39273848e-04 -2.07090788e-02\n",
      "   1.66253548e-03 -3.45233828e-03 -3.66033195e-03 -3.95458518e-03\n",
      "   1.21982321e-02]\n",
      " [-1.18811820e-02  1.10617885e-02  3.29635292e-03 -2.01881230e-02\n",
      "   3.52524919e-03 -1.47361066e-02 -4.37782379e-03 -3.06545128e-03\n",
      "   1.32545894e-02]\n",
      " [-9.03718546e-03  6.77674776e-03  4.64708079e-03 -1.59213319e-02\n",
      "  -9.21314349e-04 -5.98852476e-03 -8.00666213e-03 -5.12373913e-03\n",
      "   1.32782208e-02]\n",
      " [ 6.50403602e-03  1.11921225e-02  3.50879971e-04 -2.04772521e-02\n",
      "   2.27080332e-03 -1.40106166e-02 -6.86857104e-03 -6.10541971e-03\n",
      "   1.38620147e-02]\n",
      " [ 2.43567396e-03  9.11499932e-03  2.16444954e-04 -2.48588398e-02\n",
      "   7.07984436e-04 -9.62122064e-03 -4.89297044e-03 -9.84337740e-03\n",
      "   1.02270562e-02]\n",
      " [ 2.26277113e-03  8.35066102e-03  2.70845043e-03 -2.60457844e-02\n",
      "   5.98164648e-03 -1.18408408e-02 -2.56001437e-03 -9.61526111e-03\n",
      "   1.31630236e-02]\n",
      " [-5.84147312e-03  1.36179551e-02  1.80103839e-03 -1.52330538e-02\n",
      "  -2.01670034e-03 -1.04801049e-02 -2.95262458e-03 -2.39368412e-03\n",
      "   1.54078845e-02]\n",
      " [ 3.25497054e-03  1.26762902e-02  2.28763651e-03 -1.96174793e-02\n",
      "   3.35721276e-03 -1.28908427e-02 -3.65009112e-03 -7.66911497e-03\n",
      "   1.66470408e-02]\n",
      " [-4.28622076e-03  8.31990223e-03  7.73472665e-03 -1.54267605e-02\n",
      "   2.63830228e-03 -1.13911694e-02 -4.51950170e-03  1.61399366e-03\n",
      "   8.98787007e-03]\n",
      " [-9.43367369e-04  7.99000077e-03  5.42610651e-03 -1.96690671e-02\n",
      "  -7.15675822e-04 -1.38588948e-02 -1.08325351e-02 -2.76368274e-03\n",
      "   1.04747340e-02]\n",
      " [-5.31435478e-03  1.11042429e-02  1.54782447e-03 -1.93070527e-02\n",
      "   2.49611889e-03 -4.45470866e-03 -2.77930452e-03 -2.65400251e-03\n",
      "   1.33756809e-02]\n",
      " [-1.00619495e-02  8.26730020e-03  1.49670849e-03 -2.10605077e-02\n",
      "   5.51784458e-03 -2.75536999e-03 -2.26883264e-03 -3.76587966e-03\n",
      "   9.68785025e-03]\n",
      " [ 1.07560307e-03  1.00335982e-02  2.81338114e-03 -1.97553411e-02\n",
      "  -1.04382599e-03 -1.39044933e-02  5.45763760e-04 -8.28378461e-03\n",
      "   1.84503626e-02]\n",
      " [ 1.28104351e-04  1.17669292e-02  2.93033291e-03 -2.48824991e-02\n",
      "   4.46982123e-03 -1.41638964e-02 -8.99009872e-04 -6.97066449e-03\n",
      "   1.73095800e-02]\n",
      " [ 7.12350942e-04  1.07209245e-02  4.32725530e-04 -2.64197700e-02\n",
      "   2.07468285e-03 -1.42686432e-02 -6.52722316e-03 -4.96629067e-03\n",
      "   1.19612133e-02]\n",
      " [-7.44153885e-03  1.02522187e-02  3.00194626e-03 -1.79274660e-02\n",
      "   5.75734396e-03 -4.56768833e-03 -5.75394602e-03 -1.11149298e-03\n",
      "   4.38132975e-03]\n",
      " [-6.74981624e-04  1.12761986e-02  1.46771851e-03 -1.88392177e-02\n",
      "   2.34327954e-03 -1.26388352e-02 -4.04266827e-03 -5.19710220e-03\n",
      "   9.99249704e-03]\n",
      " [-7.16290344e-03  7.30603095e-03  4.59354371e-03 -2.16932222e-02\n",
      "   1.69292756e-03 -6.42653042e-03 -7.18693482e-03 -4.19348106e-03\n",
      "   8.38389620e-03]\n",
      " [ 5.72869368e-03  8.54730792e-03 -1.75451627e-04 -1.67428572e-02\n",
      "   3.07575939e-03 -9.08162072e-03 -1.72370416e-03 -8.05182569e-03\n",
      "   1.50342723e-02]\n",
      " [-3.78365535e-03  1.00017088e-02  1.55935483e-03 -1.99453458e-02\n",
      "  -2.78532563e-04 -1.23486258e-02 -7.10126944e-04 -6.60825521e-03\n",
      "   1.87690966e-02]\n",
      " [-7.29845092e-03  1.29134823e-02  3.88262561e-04 -1.39261605e-02\n",
      "   1.65458897e-03 -9.36684571e-03 -7.27541232e-03 -3.73725523e-03\n",
      "   1.54314395e-02]\n",
      " [-3.95007432e-03  8.02597124e-03  7.40864780e-03 -1.62617769e-02\n",
      "  -1.20293035e-03 -9.04678740e-03 -8.30045994e-03 -2.09441222e-03\n",
      "   1.04810363e-02]] (32, 90, 80, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-7cac84aa7d5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtrain_online_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-9519bb7f8b06>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, state, action, reward, new_state, done)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m             )\n\u001b[1;32m--> 848\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import time\n",
    "agent = QLearningAgent(env)  \n",
    "ep_rewards = []\n",
    "total_reward = 0\n",
    "n_steps = 1000000  # total number of training steps\n",
    "copy_steps = 5000\n",
    "save_steps = 1000 \n",
    "\n",
    "# Train the model after 4 actions\n",
    "train_online_steps = 4\n",
    "\n",
    "# Train the model after 4 actions\n",
    "# Considered as a hyperparameter\n",
    "update_target_steps = 5000\n",
    "\n",
    "done=True\n",
    "for step in range(n_steps):\n",
    "    print(\"\\r\\tAction step: {}/{} ({:.1f}%)\\tLoss: {:5f}\".format(\n",
    "        step,\n",
    "        n_steps,\n",
    "        step * 100 / n_steps, \n",
    "        agent.loss_val), end=\"\")\n",
    "\n",
    "    if done: # game over, start again\n",
    "        obs = env.reset()\n",
    "        ep_rewards.append(total_reward)\n",
    "        total_reward = 0\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "    total_perc = int(step * 100 / n_steps)\n",
    "\n",
    "    # Get a exploration/exploitation action depending on the\n",
    "    # current epsilon value\n",
    "#     q_values = agent.online.predict(state)\n",
    "    action = agent.get_action(state, step)\n",
    "\n",
    "    # Online DQN plays\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    next_state = preprocess_observation(next_obs)\n",
    "    agent.save_to_memory(state, action, reward, next_state, done)\n",
    "    \n",
    "    if step % train_online_steps == 0:\n",
    "        agent.train(state, action, reward, next_state, done)\n",
    "\n",
    "    env.render()\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    # Regularly copy the online DQN to the target DQN\n",
    "    if step % update_target_steps == 0:\n",
    "        agent.update_target()\n",
    "#     # And save regularly\n",
    "#     if step % save_steps == 0:\n",
    "#         agent.saver.save(agent.sess, agent.checkpoint_path)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8. Print the evolution of the total number of rewards w.r.t. the episodes.\n",
    "\n",
    "1.9. Analyse the behaviour of the saved agents, can you notice any emerging strategies?\n",
    "\n",
    "https://www.youtube.com/watch?v=OxOqLQ8Ed_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
